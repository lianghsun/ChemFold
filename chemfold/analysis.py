import pandas as pd
import numpy as np
import glob
import json
import re
import os, sys, logging
import argparse
from scipy.sparse import csr_matrix, coo_matrix
from pandas import Series, DataFrame
from scipy.stats import binom, fisher_exact, ks_2samp, chi2_contingency, norm
from statsmodels import robust
import scipy.io as sio


#logging
shandler = logging.StreamHandler(stream=sys.stdout)
formatter = logging.Formatter('%(asctime)s -%(name)s - %(levelname)s - %(message)s')
shandler.setFormatter(formatter)
shandler.setLevel(logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.addHandler(shandler)


# statistic functions

# Determine whether the the folds are diverging significantly in terms of their count of actives
def chi2_contingency_pval_act(row):
	try:
		u = row.unstack().astype(int)
		usum = u.sum()
		chi2 = chi2_contingency(u.loc[:,usum>0].values)
		return chi2[1]
	except:
		print(usum)
		raise

# Determine whether the distribution of datapoints per fold diverges signficantly from random
def chi2_contingency_pval_split(row):
	try:
		obs = row.unstack().sum().astype(int)
		n = len(obs)
		expected = np.repeat(np.round(obs.sum()/n).astype(int),n)
		chi2 = chi2_contingency(np.array([obs,expected]))
		return(chi2[1])
	except:
		print(obs)
		raise

# Obtain standard deviation and median absolute deviation of the datapoint fraction assigned to the folds per task
def std_fold_split(row):
	u = row.unstack().astype(int)
	usum = u.sum()
	return (usum/usum.sum()).std()

def mad_fold_split(row):
	u = row.unstack().astype(int)
	usum = u.sum()
	return robust.mad(usum/usum.sum())

# Obtain standard deviation and median absolute deviation of the fraction of actives in each of the folds per task
def std_fold_act(row):
	u = row.unstack().astype(int)
	usum = u.sum()
	u.loc[:,usum>0]
	uratio = u.loc[1,usum>0]/usum[usum>0]
	return uratio.std()

def mad_fold_act(row):
	u = row.unstack().astype(int)
	usum = u.sum()
	u.loc[:,usum>0]
	uratio = u.loc[1,usum>0]/usum[usum>0]
	return robust.mad(uratio)


#sample N pairs of compounds from the Xmatrix an calculate the tanimoto similarity for each pair
def tc_pair_sample(X, folds, Nsample):
	X1_sample = np.random.choice(X.shape[0], Nsample, replace=True)
	X2_sample = np.random.choice(X.shape[0], Nsample, replace=True)
	mask = (X1_sample != X2_sample)
	X1_sample = X1_sample[mask]
	X2_sample = X2_sample[mask]
	X1 =   np.array(X[X1_sample].todense())
	X2 =   np.array(X[X2_sample].todense())
	X1_sum = X1.sum(axis=1)
	X2_sum = X2.sum(axis=1)
	X12_sum = (X1*X2).sum(axis=1)
	tc = X12_sum/(X1_sum+X2_sum-X12_sum)
	fold_match = (folds[X1_sample] == folds[X2_sample])
	return fold_match, tc

#sample at maximimum Maxsample pairs from X in batches of size Nbatch
#bin distance into Nbins equidistant bins
#stop sampling when the change in the fraction of intra fold pairs for all distance bins drops below Precision
#and the each bin has at least Minpop intra-fold pairs
def batch_tc_pair_hist_conv(X, folds, Nbins=20, Precision=0.01, Minpop = 3, Maxsample=10000000, Nbatch=50000):
	tc_hist_same_fold = np.zeros(Nbins,dtype=int)
	tc_hist_different_fold = np.zeros(Nbins,dtype=int)
	intra_fold_fraction = np.zeros(Nbins,dtype=float)
	converged = False
	samples = 0
	bin_lower_limits = np.arange(0.0,1.0,1/Nbins)
	while ((not converged) and (samples <= Maxsample)):
		fmatch_b, tc_b  = tc_pair_sample(X, folds, Nbatch)
		tc_hist_same_fold += np.histogram(tc_b[fmatch_b],bins=Nbins,range=(0.0,1.0))[0]
		tc_hist_different_fold += np.histogram(tc_b[~fmatch_b],bins=Nbins,range=(0.0,1.0))[0]
		new_f = tc_hist_same_fold /np.clip((tc_hist_same_fold + tc_hist_different_fold),a_min=1,a_max=None)
		delta_max = np.absolute(new_f - intra_fold_fraction).max()
		samples += Nbatch
		intra_fold_fraction = new_f
		minimal_bin_population = tc_hist_same_fold.min()
		converged = (delta_max < Precision) and (minimal_bin_population >= Minpop)
		logger.info('''Samples: {0}, max change = {1}, minimal bin population {2} --> Converged: {3}, ecxceeded Maxsamples ({4}): {5}'''.format(samples,delta_max,minimal_bin_population,converged,Maxsample,samples > Maxsample))
	if not converged:
		logger.warning('Failed to reach convergence after sampling of {0} pairs'.format(samples))
	return tc_hist_same_fold, tc_hist_different_fold, intra_fold_fraction, bin_lower_limits


#folding method assessment functions
def balance(ptuner,pfold):
	"""
	This function analyzes the split imbalanace for tasks with a given fold split and a y-matrix.

	For input:
	A combination of:
	1. the y-mtrix as in generated by MELLODDY TUNER in the files_4_ml folder and 
	2. a numpy file with the folds (as for example produced by the sphere exclusion code, but also by MELLODDY TUNER using LSH in the files_4_ml folder is needed.
	This script generates the counts and will take some time for this.

	It will output:
	1. a file listing imbalanace metrics per task
	2. a file aggregating this imbalance information per task data volume 
	Both files are written to the output location of the folding method.

	The following imbalance metrics are calculated for each task
	1. Standard deviation of the fraction of data points allocated to each fold
	2. Median absolute deviation of the fraction of data points allocated to each fold
	3. Standard deviation of the fraction of +1 labels (active) within the data points allocated to each fold
	4. Median absolute deviation of the fraction of +1 labels (active) within the data points allocated to each fold
	5. Boolean flag indiating whether the task will have 10 datapoints for each label (+1 and -1) in each fold. Tasks not meeting this requirment are exclded from performance evaluotation
	6. p-value from chi2 statistics comparing the number of datapoints allocated to each fold with result of an even distribution as expected from a random split. This will detect whether an imbalance indicated by 1. is statistically significant 
	7. p-value from chi2 statistics comparing the number of actives and inactives allocated to each fold. This will detect whether an imbalance as indicated by 3. is staistically significant.
	8. a combination boolean split_inbalanced flag indicating a substantial deviation of > 0.5 in 1. and a significant p-value as in 6. below 0.05
	9. a combination boolean act_inbalanced flag indicating a substantial deviation of > 0.5 in 3. and a significant p-value as in 7. below 0.05

	"""

	print('Data and label balance analysis started')

	# Read fold split counts
	# read from the y matrix and the fold vector
	y_mat = sio.mmread(os.path.join(ptuner,'files_4_ml','T10_y.mtx'))
	logger.info('read in y data')

	# make a sparse datafrme from the csr matrix
	y_df_sp = pd.DataFrame.sparse.from_spmatrix(y_mat)

	foldings = glob.glob(os.path.join(pfold,'*folds.npy'))
	
	for method in foldings:
		prefix = os.path.split(method)[1].split('_')[0]
		folds = np.load(os.path.join(pfold,prefix+'_folds.npy'))
		logger.info('read in fold data for {0}'.format(prefix))
		print('Running imbalance check on: ' + prefix)

		# determine the number of folds from the folds file
		fold_vals = np.arange(folds.max()+1)

		# iteratite over the folds and over the two possible values +1 / -1 and do the count statistics. This hasn't been optimized and will take some time. It might be faster, if coversion to a dense dataframe were to be used
		count_ser = pd.concat({f:pd.concat({1:(y_df_sp.loc[folds==f] == 1).sum(axis=0),-1:(y_df_sp.loc[folds==f] == -1).sum(axis=0)}) for f in fold_vals})
		count_df = count_ser.reset_index()
		count_df.columns = ['fold_id','class_label','cont_classification_task_id','label_counts']

		# The follwing operation merges clustering information with cluster to fold mapping only the fold_data series to be returned to client. On top of this the server communicates the overall fold ratios.
		count_unstacked = count_df.set_index(['cont_classification_task_id','class_label','fold_id']).sort_index().unstack(['class_label','fold_id'])
		count_unstacked['sum'] = count_unstacked['label_counts'].sum(axis=1)
		count_unstacked['sum_neg'] = count_unstacked[('label_counts',-1)].sum(axis=1)
		count_unstacked['sum_pos'] = count_unstacked[('label_counts',1)].sum(axis=1)
		count_unstacked['pos_rate'] = count_unstacked['sum_pos'] / count_unstacked['sum']


		# Apply statistic functions
		count_unstacked['chi2_contingency_p_split'] = count_unstacked['label_counts'].apply(chi2_contingency_pval_split,axis=1)
		count_unstacked['chi2_contingency_p_act'] = count_unstacked['label_counts'].apply(chi2_contingency_pval_act,axis=1)
		count_unstacked['below_10'] = (count_unstacked['label_counts'] < 10).any(axis=1)
		count_unstacked['below_05'] = (count_unstacked['label_counts'] < 5).any(axis=1)
		count_unstacked['std_fold_split'] = count_unstacked['label_counts'].apply(std_fold_split,axis=1)
		count_unstacked['mad_fold_split'] = count_unstacked['label_counts'].apply(mad_fold_split,axis=1)
		count_unstacked['std_fold_act'] = count_unstacked['label_counts'].apply(std_fold_act,axis=1)
		count_unstacked['mad_fold_act'] = count_unstacked['label_counts'].apply(mad_fold_act,axis=1)
		sum_labels = np.power(10,np.arange(1,np.ceil(np.log10(count_unstacked['sum'].max()))))
		sum_bins= np.power(10,np.arange(1,np.ceil(np.log10(count_unstacked['sum'].max()))+1))
		count_unstacked['size_bin_lower_limit'] = pd.cut(count_unstacked['sum'],bins=sum_bins,labels=sum_labels)

		summary_data = count_unstacked.drop('label_counts',axis=1)
		summary_data.columns = summary_data.columns.droplevel(['class_label','class_label'])

		# For assigning the category unbalanced_split/unbalanced_act we require statistical signficance and a meaningfull effect size, that is standard_deviations
		summary_data['chi2_contingency_p_split_signif'] = (summary_data['chi2_contingency_p_split']<0.05)
		summary_data['chi2_contingency_p_act_signif'] = (summary_data['chi2_contingency_p_act']<0.05)
		summary_data['split_unbalanced'] = (summary_data['chi2_contingency_p_split']<0.05) & (summary_data['std_fold_split']>0.05)
		summary_data['act_unbalanced'] = (summary_data['chi2_contingency_p_act']<0.05) & (summary_data['std_fold_act']>0.05)


		# Create a aggregate fractions by size bin
		sizes = summary_data.groupby(['size_bin_lower_limit']).size()
		stats_by_size = pd.concat({'fraction_below_10': summary_data[summary_data['below_10']==True].groupby(['size_bin_lower_limit']).size()/sizes,
				                   'fraction_below_05': summary_data[summary_data['below_05']==True].groupby(['size_bin_lower_limit']).size()/sizes,
		'fraction_act_unbalanced': summary_data[summary_data['act_unbalanced']==True].groupby(['size_bin_lower_limit']).size()/sizes,
		'fraction_split_unbalanced': summary_data[summary_data['split_unbalanced']==True].groupby(['size_bin_lower_limit']).size()/sizes},axis=1)

		stats_by_size['mean_std_fold_act'] = summary_data.groupby('size_bin_lower_limit')['std_fold_act'].mean()
		stats_by_size['mean_std_fold_split'] = summary_data.groupby('size_bin_lower_limit')['std_fold_split'].mean()

		stats_by_size['count'] = sizes
		logger.info('aggregated by size bin')

		# Write out results
		summary_data.to_csv(os.path.join(pfold,prefix+'_split_stats.txt'), sep='\t')
		stats_by_size.to_csv(os.path.join(pfold,prefix+'_split_stats_overview.txt'), sep='\t')

		print('Imbalance analysis results (split_stats.txt, split_stats_overview.txt) were written to: ' + pfold + ' .')
		logger.info('DONE')


def performance(psc, baseline_prefix, pfold):
	"""
	This function analyzes the perfromance difference of folding method compared to a baseline  fold splitting.

	For input:
	1. path to the sparsechem folder which includes subfolders for every folding method to be analyzed including the models folder and the performance json file 

	It will output:
	1. a absolute performance summary csv
	2. a delta performance summary csv
	3. mean delta perfromances for size bins

	"""

	print('Performance analysis started')

	#get raw results and do calculations
	fnames = glob.glob(os.path.join(psc,'*.json'))


	results_agg = {}
	results_agg2 = {}
	results = {}
	conf = {}
	conf2 = {}

	for ix, fname in enumerate(fnames):
		f = open(fname, 'r')
		data = json.load(f)
		results_agg[ix] = pd.read_json(data['validation']['classification_agg'], typ='series')
		results[ix] = pd.read_json(data['validation']['classification'])

		conf[ix] = pd.Series(data['conf'])
		conf[ix]['fname'] = fname

		prefix = conf[ix]['prefix']
		logger.info('read in performance data for {0}'.format(prefix))
		print('Running performance analysis on: ' + prefix)

	conf_df = pd.concat(conf,axis=0).unstack()
	conf_df.index.names = ['model_index']

	results_df = pd.concat(results)
	results_df.index.names = ['model_index','continuous_task_id']
	results_agg_df = pd.concat(results_agg).unstack()
	results_agg_df.index.names = ['model_index']
	logger.info('read in performance data')

	mean = results_agg_df.join(conf_df[['prefix','fold_va']]).groupby('prefix').mean()
	mean_delta = mean.loc[mean.index != baseline_prefix] - mean.loc[baseline_prefix]
	std = results_agg_df.join(conf_df[['prefix','fold_va']]).groupby('prefix').std()
	z = mean_delta/np.sqrt(std.loc[std.index != baseline_prefix]**2 + std.loc[baseline_prefix]**2)
	p_val_worse = pd.DataFrame(norm.cdf(z),columns=z.columns,index=z.index)

	score_cols = ['roc_auc_score','auc_pr','avg_prec_score','f1_max','kappa']

	result_agg_delta = pd.concat({'delta_basline_mean': mean_delta[score_cols], 'p-value':p_val_worse[score_cols]},axis=1,sort=True)

	res_detail = results_df.join(conf_df[['prefix','fold_va']]).reset_index().set_index(['prefix','continuous_task_id','fold_va']).drop('model_index',axis=1)
	res_detail_mean = res_detail[score_cols].groupby(level=['prefix','continuous_task_id'])[score_cols].mean()
	res_detail_std = res_detail.groupby(level=['prefix','continuous_task_id'])[score_cols].std()
	mean_detail_delta = res_detail_mean.loc[res_detail_mean.index.get_level_values('prefix') != baseline_prefix] - res_detail_mean.loc[baseline_prefix]
	z_detail = mean_detail_delta/np.sqrt(res_detail_std.loc[res_detail_std.index.get_level_values('prefix') != baseline_prefix]**2 + res_detail_std.loc[baseline_prefix]**2)
	p_val_worse_detail = pd.DataFrame(norm.cdf(z_detail),columns=z_detail.columns,index=z_detail.index)

	count_cols = ['num_pos','num_neg']
	counts = res_detail[count_cols].groupby(level=['prefix','continuous_task_id']).sum()
	counts['below_10'] = (res_detail[count_cols].groupby(level=['prefix','continuous_task_id']).min()<10).any(axis=1)
	counts['total'] = counts['num_pos'] + counts['num_neg']

	result_detail_delta = pd.concat({'delta_baseline_mean': mean_detail_delta, \
		                           'p-value': p_val_worse_detail,\
		                         'counts':counts.loc[counts.index.get_level_values('prefix') != baseline_prefix]},axis=1,sort=True)

	sum_labels = np.power(10,np.arange(1,np.ceil(np.log10(result_detail_delta[('counts','total')].max()))))
	sum_bins= np.power(10,np.arange(1,np.ceil(np.log10(result_detail_delta[('counts','total')].max()))+1))
	result_detail_delta[('counts','size_bin_lower_limit')] = pd.cut(result_detail_delta[('counts','total')],bins=sum_bins,labels=sum_labels).astype(int)

	counts_total = result_detail_delta.reset_index().groupby(['prefix',('counts','size_bin_lower_limit')]).size()
	counts_worse = result_detail_delta.loc[result_detail_delta[('p-value','auc_pr')]<0.05].reset_index().groupby(['prefix',('counts','size_bin_lower_limit')]).size()

	fraction_worse = counts_worse.reindex(counts_total.index).fillna(0)/counts_total
	mean_delta_aucpr = result_detail_delta.reset_index().groupby(['prefix',('counts','size_bin_lower_limit')]).mean()[('delta_baseline_mean','auc_pr')]


	# Write out results
	result_agg_delta.to_csv(os.path.join(pfold,'performance_summary_total.txt'),sep='\t')

	result_detail_delta_4csv = result_detail_delta.copy()
	result_detail_delta_4csv.columns = [' '.join(col).strip() for col in result_detail_delta_4csv.columns.values]
	result_detail_delta_4csv.to_csv(os.path.join(pfold,'performance_delta_detail.txt'),sep='\t')


	summary_by_size = pd.concat({'mean_delta_auc_pr':mean_delta_aucpr,'fraction_significantly_worse':fraction_worse},axis=1)
	summary_by_size.index.names = ['split_method','size_bin_lower_limit']
	summary_by_size.to_csv(os.path.join(pfold,'performance_summary_by_size.txt'),sep='\t')

	print('Performance analysis results (performance_summary_total.txt, performance_delta_detail.txt, performance_summary_by_size.txt) were written to: ' + pfold + '.')
	logger.info('DONE')


def similarity(ptuner,pfold,maxsample,batchsize,numbins,precision,minpop,rseed):

	"""
	This function creates distance histograms. Sampling will take some time please be patient.

	For input:
	1. path to the matrix of ECFP features [compounds x ecfp], .mtx file

	It will output:
	1. a absolute performance summary csv
	2. a delta performance summary csv
	3. mean delta perfromances for size bins

	"""

	print('Similarity analysis started')

	# Read input files: folds and fingerprints (ecfp6)
	logger.info('read in X data (ecfp)')
	X = sio.mmread(os.path.join(ptuner,'files_4_ml','T11_x.mtx'))
	X = X.tocsr()

	foldings = glob.glob(os.path.join(pfold,'*folds.npy'))
	
	for method in foldings:
		prefix = os.path.split(method)[1].split('_')[0]
		folds = np.load(os.path.join(pfold,prefix+'_folds.npy'))
		logger.info('read in fold data for {0}'.format(prefix))
		print('Running similarity analysis on: ' + prefix)

		#set the random seed
		np.random.seed(rseed)

		#run the sampling
		tc_hist_same_fold, tc_hist_different_fold, intra_fold_fraction, bin_lower_limits = \
			batch_tc_pair_hist_conv(X, folds, Nbins=numbins, Precision=precision, Minpop=minpop, Maxsample = maxsample, Nbatch = batchsize)
		logger.info('finished pair sampling')
		#calculate significance from deviation of expected random value for Fraction_Intra_fold (0.2 for 5 folds)
		p_intra_fold = tc_hist_same_fold.sum() / (tc_hist_same_fold.sum() + tc_hist_different_fold.sum())
		binom_pval = binom.logsf(k=tc_hist_same_fold.astype(int)-1,n=tc_hist_same_fold.astype(int)+tc_hist_different_fold.astype(int), p=p_intra_fold)
		hist_df = pd.DataFrame({'tc_similarity_bin': bin_lower_limits, 'Num_intra_fold_pairs':tc_hist_same_fold, \
				                'Num_inter_fold_pairs': tc_hist_different_fold, 'Fraction_Intra_fold':intra_fold_fraction, \
				                '1-binom.logsf':binom_pval})
		#write out results
		hist_df.to_csv(os.path.join(pfold,prefix+'_similarity_histogram.txt'),sep='\t',header=True)
		print('Similarity analysis result was written to: ' + os.path.join(pfold,prefix+'_similarity_histogram.txt') + ' .')
		logger.info('DONE')

